exp_name: 'random-experiment' # Name of Experiment
gym_id: 'Adventure-v0' # ID of Gym Environment
learning_rate: 0.001 # Learning rate for the optimizer
seed: 1 # Seed for the experiment
total_timesteps: 50000 # Total number of timesteps

torch_det: True # If toggled, torch.backend.cudnn.determentistic=False
cuda: True # Enable or disable cuda

track: False # use wandb
wandb_project_name: 'mz-planning'
wandb_entity: None
capture_video: False # capture video of the env during experiment

initial_epsilon: 0.99
decay_parameter: 0.99

num_envs: 4 # Number of parallel gym envs
num_steps: 128 # Number of steps to run each environment per policy rollout
anneal_lr: True # Enable learning rate annealing
gae: True # Toggele general advantage estimation
gamma: 0.99 # Discount factor
gae_lambda: 0.95 # lambda for GAE
num_minibatches: 4 # Number of mini-batches
update_epochs: 4 # The k epochs to update policy
norm_adv: True # Toggle advantage normalization
clip_coef: 0.2 # The surrogate clipping coefficient
clip_vloss: True # Toggle the use of clipped value loss
ent_coef: 0.01 # The coefficient of entropy
vf_coef: 0.5 # The coefficient of value function
max_grad_norm: 0.5 # The maximum norm for gradient clipping
target_kl: None # The target KL divergence threshold
early_stop: none # The global step at which to stop the training